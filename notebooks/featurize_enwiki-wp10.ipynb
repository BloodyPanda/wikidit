{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the wikicode in the WP10 subset of data and extract features from the wikitext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwparserfromhell as mwparser\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def num_headings(text, level):\n",
    "    \"\"\"Count number of headings in wikicode document with ``level``\"\"\"\n",
    "    return len([x for x in text.filter_headings() if x.level == level])\n",
    "\n",
    "def clean_template_name(x):\n",
    "    \"\"\"Return the cleaned and standardized template name\"\"\"\n",
    "    return str(x.name).strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "def match_template(x, pattern):\n",
    "    \"\"\"Does the object match ``pattern``\"\"\"\n",
    "    return bool(re.match(pattern, clean_template_name(x), re.I))\n",
    "\n",
    "def wikilink_title_matches(pattern, link):\n",
    "    \"\"\"Does wikilink title match ``pattern``\"\"\"\n",
    "    return bool(re.match(pattern, str(link.title), re.I))\n",
    "\n",
    "def featurize(revision):\n",
    "    \"\"\"Create features for each revision\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    revision: `dict`\n",
    "        A dict with revision information. It must contain a\n",
    "        \"wikitext\" key. It will returne\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    None\n",
    "        The dictionary ``revision`` is altered in place.\n",
    "    \n",
    "    \"\"\"\n",
    "    text = mwparser.parse(revision['wikitext'])\n",
    "\n",
    "    # number of characters\n",
    "    revision['chars'] = len(str(text))\n",
    "\n",
    "    # Content characters are visible characters. Operationalized as characters after\n",
    "    # mwparserfromhell calls the ``strip_code()`` method.\n",
    "    plaintext = text.strip_code()\n",
    "    revision['content_chars'] = len(plaintext)\n",
    "        \n",
    "    # Filter external\n",
    "    revision['external_links'] = len(text.filter_external_links())\n",
    "\n",
    "    # Total number of headings\n",
    "    headings = text.filter_headings()\n",
    "    revision['headings'] = len([x for x in headings if x.level == 2])\n",
    "    \n",
    "    # Sub-headings\n",
    "    revision['sub_headings'] = len([x for x in headings if x.level > 2])\n",
    "\n",
    "    # Number of wikilinks\n",
    "    wikilinks = text.filter_wikilinks()\n",
    "    \n",
    "    # number of images\n",
    "    revision['images'] = sum([wikilink_title_matches(r\"file|image\\:\", link) for link in wikilinks])\n",
    "\n",
    "    # number of categories\n",
    "    revision['categories'] = sum([wikilink_title_matches(\"category\\:\", link) for link in wikilinks])\n",
    "    \n",
    "    # Other wikilinks\n",
    "    revision['wikilinks'] = len(wikilinks) - sum(revision[k] for k in (\"images\", \"categories\"))\n",
    "\n",
    "    # Templates\n",
    "    templates = text.filter_templates()\n",
    "    \n",
    "    # number of who templates\n",
    "    revision['who_templates'] = sum([match_template(x, 'who$') for x in templates])\n",
    "\n",
    "    # main templates\n",
    "    revision['main_templates'] = sum([match_template(x, \"main$\") for x in templates])\n",
    "    \n",
    "    # citation templates\n",
    "    revision['cite_templates'] = sum([match_template(x, \"cite\") for x in templates])\n",
    "    \n",
    "    # has infobox\n",
    "    revision['infoboxes'] = sum([match_template(x, \"infobox\") for x in templates])\n",
    "\n",
    "    # number of citation needed templates\n",
    "    revision['citation_needed'] = sum(match_template(x, \"citation_needed|cn|fact\") for x in templates)\n",
    "    \n",
    "    # other templates\n",
    "    revision['other_templates'] = len(templates) - sum(revision[k] for k in (\"infoboxes\", \"cite_templates\", \"citation_needed\",\n",
    "                                                                             \"main_templates\", \"who_templates\"))\n",
    "    \n",
    "    # number of ref tags\n",
    "    revision['ref'] = len([x for x in text.filter_tags() if x.tag == \"ref\"])\n",
    "\n",
    "    # number of smartlists (e.g. wikitables)\n",
    "    revision['smartlists'] = len([x for x in text.nodes if isinstance(x, mwparser.smart_list.SmartList)])\n",
    "    \n",
    "    # sections\n",
    "    sections = text.get_sections(flat=True, include_lead=True, include_headings=False)\n",
    "    non_lead_paras = 0\n",
    "    non_ref_paras = 0\n",
    "    non_ref_paras_length = 0\n",
    "    for i, section in enumerate(sections):\n",
    "        paras = [mwparser.parse(x) for x in re.split(r'\\n{2,}', str(section))]\n",
    "        if (i == 0):\n",
    "            revision['lead_paras'] = len(paras)\n",
    "            revision['lead_char'] = len(str(section))\n",
    "        else:\n",
    "            non_lead_paras += 1\n",
    "            for para in paras:\n",
    "                if not len([x for x in text.filter_tags() if x.tag == \"ref\"]):\n",
    "                    non_ref_paras += 1\n",
    "                    non_ref_paras_length += len(str(para))\n",
    "    revision['non_lead_paras'] = non_lead_paras\n",
    "    revision['non_ref_paras'] = non_ref_paras\n",
    "    \n",
    "    # geotagged\n",
    "    revision['coordinates'] = '#coordinates' in str(text).lower()\n",
    "    \n",
    "    # Add plaintext for more features\n",
    "    revision['text'] = plaintext\n",
    "    del revision['wikitext']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:   27.4s\n",
      "[Parallel(n_jobs=7)]: Done 436 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=7)]: Done 786 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=7)]: Done 1236 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=7)]: Done 1858 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=7)]: Done 3729 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=7)]: Done 4479 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=7)]: Done 5329 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=7)]: Done 6279 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=7)]: Done 7329 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=7)]: Done 8479 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=7)]: Done 9729 tasks      | elapsed: 15.4min\n",
      "[Parallel(n_jobs=7)]: Done 11079 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=7)]: Done 12529 tasks      | elapsed: 19.2min\n",
      "[Parallel(n_jobs=7)]: Done 14079 tasks      | elapsed: 20.8min\n",
      "[Parallel(n_jobs=7)]: Done 15729 tasks      | elapsed: 22.5min\n",
      "[Parallel(n_jobs=7)]: Done 17498 tasks      | elapsed: 24.0min\n",
      "[Parallel(n_jobs=7)]: Done 20234 tasks      | elapsed: 25.7min\n",
      "[Parallel(n_jobs=7)]: Done 24778 tasks      | elapsed: 27.7min\n",
      "[Parallel(n_jobs=7)]: Done 32424 out of 32424 | elapsed: 28.5min finished\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "\n",
    "input_file = \"../data/enwiki.labeling_revisions.w_text.nettrom_30k.ndjson.gz\"\n",
    "workers = cpu_count() - 1\n",
    "n = None\n",
    "\n",
    "def load_and_featurize(line):\n",
    "    revision = json.loads(line)\n",
    "    featurize(revision)\n",
    "    return revision\n",
    "\n",
    "with gzip.open(input_file, \"rt\") as f:\n",
    "    pool = Parallel(n_jobs=workers, verbose=True)\n",
    "    lines = itertools.islice(iter(f), n)\n",
    "    revisions = pool(delayed(load_and_featurize)(line) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisions = pd.DataFrame.from_records(revisions)\n",
    "revisions = revisions.set_index('revid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"../data/enwiki.labeling_revisions.w_features.nettrom_30k.csv.gz\"\n",
    "revisions.to_csv(output_file, index=True, compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
