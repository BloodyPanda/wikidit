{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn_pandas import cross_val_score, DataFrameMapper\n",
    "from pandas import Categorical\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import mwapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikidit.models import featurize, load_wp10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"../data/enwiki.labeling_revisions.w_features.nettrom_30k.csv.gz\"\n",
    "revisions = load_wp10(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "sqrt_cols = ['words',\n",
    "             'headings',\n",
    "             'sub_headings',\n",
    "             'images',\n",
    "             'categories',\n",
    "             'wikilinks',\n",
    "\n",
    "             'who_templates',\n",
    "             'main_templates',\n",
    "             'cite_templates',\n",
    "             # infobox as a binary\n",
    "             'citation_needed',\n",
    "             'other_templates',\n",
    "\n",
    "             'ref',\n",
    "             'smartlists',\n",
    "             'coordinates']\n",
    "\n",
    "binarized_cols = ['coordinates', 'infoboxes']\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    (sqrt_cols, FunctionTransformer(func=np.sqrt)),\n",
    "    (binarized_cols, FunctionTransformer(func=lambda x: x.astype(bool)))\n",
    "])\n",
    "\n",
    "# clf = LogisticRegressionCV(multi_class='multinomial', \n",
    "#                           random_state=1234, \n",
    "#                           penalty='l2',\n",
    "#                           fit_intercept=True, \n",
    "#                           n_jobs=-1)\n",
    "clf = xgb.XGBClassifier(max_depth=5, learning_rate=0.01, n_estimators=100, silent=True, objective='binary:logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.utils import Parallel, delayed\n",
    "from sklearn.utils.validation import has_fit_parameter, check_is_fitted\n",
    "from sklearn.utils.metaestimators import _BaseComposition\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "\n",
    "def _parallel_fit_estimator(estimator, X, y, cat):\n",
    "    \"\"\"Private function used to fit an estimator to a class within a job.\"\"\"\n",
    "    touse = (y >= cat)\n",
    "    y_transformed = y[touse] > cat\n",
    "    estimator.fit(X[touse, :], y_transformed)\n",
    "    return estimator\n",
    "\n",
    "\n",
    "class OrdinalClassifier(_BaseComposition, ClassifierMixin, TransformerMixin):\n",
    "\n",
    "    def __init__(self, estimator, n_jobs=None, proba_transform=None, left=True):\n",
    "        self.estimator = estimator\n",
    "        self.n_jobs = n_jobs\n",
    "        self.proba_transform = proba_transform\n",
    "        self.left = left\n",
    "\n",
    "    @property\n",
    "    def named_estimators(self):\n",
    "        return Bunch(**dict(self.estimator))\n",
    "\n",
    "    def fit(self, X, y, categories='auto'):\n",
    "        if not (isinstance(y, pd.Series) and hasattr(y, \"cat\")):\n",
    "            raise ValueError(\"y must be pd.Series object with dtype Categorical\")\n",
    "\n",
    "        # this is hard-coded for categorical variables\n",
    "        self.classes_ = y.cat.categories\n",
    "\n",
    "        categories = self.classes_[:-1]\n",
    "    \n",
    "        # order of estimators\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(_parallel_fit_estimator)(clone(self.estimator), X, y, cat)\n",
    "                for cat in categories)\n",
    "\n",
    "        self.named_estimators_ = Bunch(**dict())\n",
    "        for k, e in zip(self.classes_[:-1], self.estimators_):\n",
    "            self.named_estimators_[k] = e\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        out = np.argmax(self.predict_proba(X), axis=1)\n",
    "        out = pd.Categorical.from_codes(out, categories=self.classes_, ordered=True)\n",
    "        return out\n",
    "\n",
    "    def _collect_log_probas(self, X):\n",
    "        \"\"\"Collect results from clf.predict calls. \"\"\"\n",
    "        if hasattr(clf, \"predict_log_proba\"):\n",
    "            return [clf.predict_log_proba(X) for clf in self.estimators_]\n",
    "        else:\n",
    "            return [np.log(clf.predict_proba(X)) for clf in self.estimators_]\n",
    "\n",
    "    def _predict_log_proba(self, X):\n",
    "        \"\"\"Predict log class probabilities for X\"\"\"\n",
    "        out = np.empty((X.shape[0], len(y.cat.categories)))\n",
    "        for i, logp in enumerate(self._collect_log_probas(X)):\n",
    "            if i > 0:\n",
    "                # add log conditional probability\n",
    "                logp += out[:, (i, )]\n",
    "            out[:, i:(i + 2)] = logp\n",
    "        return out\n",
    "\n",
    "    @property\n",
    "    def predict_log_proba(self):\n",
    "        \"\"\"Compute probabilities of possible outcomes for samples in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        Returns\n",
    "        ----------\n",
    "        avg : array-like, shape = [n_samples, n_classes]\n",
    "            Weighted average probability for each class per sample.\n",
    "        \"\"\"\n",
    "        return self._predict_log_proba\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return np.exp(self.predict_log_proba(X))\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.proba_transform:\n",
    "            return self.predict_proba(X)\n",
    "        else:\n",
    "            return self.predict(X)\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\" Setting the parameters for the voting classifier\n",
    "        Valid parameter keys can be listed with get_params().\n",
    "        Parameters\n",
    "        ----------\n",
    "        **params : keyword arguments\n",
    "            Specific parameters using e.g. set_params(parameter_name=new_value)\n",
    "            In addition, to setting the parameters of the ``VotingClassifier``,\n",
    "            the individual classifiers of the ``VotingClassifier`` can also be\n",
    "            set or replaced by setting them to None.\n",
    "        Examples\n",
    "        --------\n",
    "        # In this example, the RandomForestClassifier is removed\n",
    "        clf1 = LogisticRegression()\n",
    "        clf2 = RandomForestClassifier()\n",
    "        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)]\n",
    "        eclf.set_params(rf=None)\n",
    "        \"\"\"\n",
    "        super(OrdinalClassifier, self)._set_params('estimator', **params)\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return super(OrdinalClassifier, self)._get_params('estimator', deep=deep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaults for now\n",
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('clf', OrdinalClassifier(clf, n_jobs=4))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model on Full Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "fitted = pipe.fit(X=revisions.copy(), y=revisions['wp10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20093"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(fitted.predict(revisions.copy()) == revisions['wp10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/model-xgboost-ordinal.pkl\", \"wb\")  as f:\n",
    "    dill.dump(fitted, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.2,  0.3],\n",
       "       [ 0.1,  0.2,  0.3]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0.5, 0.2, 0.3], [0.1, 0.2, 0.3]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Peformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Cross validate\n",
    "2. Out of sample\n",
    "3. Other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
